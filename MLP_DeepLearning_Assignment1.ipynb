{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "mEeYiv7ez-g6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('diabetes.csv')"
      ],
      "metadata": {
        "id": "PbTUqe0Zz2Cc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop('Outcome', axis=1).values\n",
        "y = data['Outcome'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_train = torch.FloatTensor(y_train)\n",
        "y_test = torch.FloatTensor(y_test)\n",
        "\n",
        "\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "deR3jyfizy6d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "613I2mqKzwRN",
        "outputId": "138902ba-9272-433a-873f-e11dcbec13b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Loss: 0.7000212669372559\n",
            "Epoch [2/200], Loss: 0.6463653445243835\n",
            "Epoch [3/200], Loss: 0.6444919109344482\n",
            "Epoch [4/200], Loss: 0.6578204035758972\n",
            "Epoch [5/200], Loss: 0.5467489361763\n",
            "Epoch [6/200], Loss: 0.5639954209327698\n",
            "Epoch [7/200], Loss: 0.7988976836204529\n",
            "Epoch [8/200], Loss: 0.542756199836731\n",
            "Epoch [9/200], Loss: 0.5571168065071106\n",
            "Epoch [10/200], Loss: 0.7009866833686829\n",
            "Epoch [11/200], Loss: 0.7764363288879395\n",
            "Epoch [12/200], Loss: 0.7050511240959167\n",
            "Epoch [13/200], Loss: 0.5990083813667297\n",
            "Epoch [14/200], Loss: 0.5183179378509521\n",
            "Epoch [15/200], Loss: 0.4007430970668793\n",
            "Epoch [16/200], Loss: 0.6280878782272339\n",
            "Epoch [17/200], Loss: 0.5005929470062256\n",
            "Epoch [18/200], Loss: 0.5375706553459167\n",
            "Epoch [19/200], Loss: 0.7155391573905945\n",
            "Epoch [20/200], Loss: 0.49015799164772034\n",
            "Epoch [21/200], Loss: 0.8578360080718994\n",
            "Epoch [22/200], Loss: 0.48283496499061584\n",
            "Epoch [23/200], Loss: 0.3581743538379669\n",
            "Epoch [24/200], Loss: 0.23918698728084564\n",
            "Epoch [25/200], Loss: 0.2546110153198242\n",
            "Epoch [26/200], Loss: 0.3504028022289276\n",
            "Epoch [27/200], Loss: 0.680308997631073\n",
            "Epoch [28/200], Loss: 0.5196459889411926\n",
            "Epoch [29/200], Loss: 0.607330858707428\n",
            "Epoch [30/200], Loss: 0.6207520365715027\n",
            "Epoch [31/200], Loss: 0.5006275773048401\n",
            "Epoch [32/200], Loss: 0.7165579199790955\n",
            "Epoch [33/200], Loss: 0.38240253925323486\n",
            "Epoch [34/200], Loss: 0.23730291426181793\n",
            "Epoch [35/200], Loss: 0.47510555386543274\n",
            "Epoch [36/200], Loss: 0.44652271270751953\n",
            "Epoch [37/200], Loss: 0.2724328339099884\n",
            "Epoch [38/200], Loss: 0.5211106538772583\n",
            "Epoch [39/200], Loss: 0.5250411629676819\n",
            "Epoch [40/200], Loss: 0.15628311038017273\n",
            "Epoch [41/200], Loss: 0.5712621212005615\n",
            "Epoch [42/200], Loss: 0.2995985448360443\n",
            "Epoch [43/200], Loss: 0.6154395937919617\n",
            "Epoch [44/200], Loss: 0.45134082436561584\n",
            "Epoch [45/200], Loss: 0.9798541069030762\n",
            "Epoch [46/200], Loss: 0.34458673000335693\n",
            "Epoch [47/200], Loss: 0.437080979347229\n",
            "Epoch [48/200], Loss: 0.4846173822879791\n",
            "Epoch [49/200], Loss: 0.5434627532958984\n",
            "Epoch [50/200], Loss: 0.41384634375572205\n",
            "Epoch [51/200], Loss: 0.6745303273200989\n",
            "Epoch [52/200], Loss: 0.29635944962501526\n",
            "Epoch [53/200], Loss: 0.3975657522678375\n",
            "Epoch [54/200], Loss: 0.2887512445449829\n",
            "Epoch [55/200], Loss: 0.6953487992286682\n",
            "Epoch [56/200], Loss: 1.0315712690353394\n",
            "Epoch [57/200], Loss: 0.2915601432323456\n",
            "Epoch [58/200], Loss: 0.36018943786621094\n",
            "Epoch [59/200], Loss: 0.47810593247413635\n",
            "Epoch [60/200], Loss: 0.4359699785709381\n",
            "Epoch [61/200], Loss: 0.608176052570343\n",
            "Epoch [62/200], Loss: 0.3565167188644409\n",
            "Epoch [63/200], Loss: 0.05311592295765877\n",
            "Epoch [64/200], Loss: 0.3064008355140686\n",
            "Epoch [65/200], Loss: 0.43420013785362244\n",
            "Epoch [66/200], Loss: 0.4571826159954071\n",
            "Epoch [67/200], Loss: 0.3526867926120758\n",
            "Epoch [68/200], Loss: 0.2606108486652374\n",
            "Epoch [69/200], Loss: 0.1215495839715004\n",
            "Epoch [70/200], Loss: 0.6878294348716736\n",
            "Epoch [71/200], Loss: 0.22555656731128693\n",
            "Epoch [72/200], Loss: 0.46439191699028015\n",
            "Epoch [73/200], Loss: 0.7271947264671326\n",
            "Epoch [74/200], Loss: 0.2902742326259613\n",
            "Epoch [75/200], Loss: 0.6974464058876038\n",
            "Epoch [76/200], Loss: 0.2974283695220947\n",
            "Epoch [77/200], Loss: 0.5806031823158264\n",
            "Epoch [78/200], Loss: 0.2585819363594055\n",
            "Epoch [79/200], Loss: 0.3133072853088379\n",
            "Epoch [80/200], Loss: 0.30872488021850586\n",
            "Epoch [81/200], Loss: 0.42532241344451904\n",
            "Epoch [82/200], Loss: 0.28457799553871155\n",
            "Epoch [83/200], Loss: 0.4512271583080292\n",
            "Epoch [84/200], Loss: 0.3103123605251312\n",
            "Epoch [85/200], Loss: 0.777012825012207\n",
            "Epoch [86/200], Loss: 0.3654688596725464\n",
            "Epoch [87/200], Loss: 0.7034716606140137\n",
            "Epoch [88/200], Loss: 0.18374685943126678\n",
            "Epoch [89/200], Loss: 0.4787794053554535\n",
            "Epoch [90/200], Loss: 0.6414262652397156\n",
            "Epoch [91/200], Loss: 0.3052760362625122\n",
            "Epoch [92/200], Loss: 0.3390067517757416\n",
            "Epoch [93/200], Loss: 0.17252039909362793\n",
            "Epoch [94/200], Loss: 0.5414543747901917\n",
            "Epoch [95/200], Loss: 0.2777308523654938\n",
            "Epoch [96/200], Loss: 0.13159151375293732\n",
            "Epoch [97/200], Loss: 0.4492167532444\n",
            "Epoch [98/200], Loss: 0.37244927883148193\n",
            "Epoch [99/200], Loss: 0.42955514788627625\n",
            "Epoch [100/200], Loss: 0.25070643424987793\n",
            "Epoch [101/200], Loss: 0.13514386117458344\n",
            "Epoch [102/200], Loss: 0.2573881149291992\n",
            "Epoch [103/200], Loss: 0.3658714294433594\n",
            "Epoch [104/200], Loss: 0.2981013357639313\n",
            "Epoch [105/200], Loss: 0.8636013865470886\n",
            "Epoch [106/200], Loss: 0.5558617115020752\n",
            "Epoch [107/200], Loss: 1.0080596208572388\n",
            "Epoch [108/200], Loss: 0.5508338809013367\n",
            "Epoch [109/200], Loss: 0.5439394116401672\n",
            "Epoch [110/200], Loss: 0.4424832761287689\n",
            "Epoch [111/200], Loss: 0.15631943941116333\n",
            "Epoch [112/200], Loss: 0.2384006530046463\n",
            "Epoch [113/200], Loss: 0.769586980342865\n",
            "Epoch [114/200], Loss: 0.32064083218574524\n",
            "Epoch [115/200], Loss: 0.1706138700246811\n",
            "Epoch [116/200], Loss: 0.2010771781206131\n",
            "Epoch [117/200], Loss: 0.33749890327453613\n",
            "Epoch [118/200], Loss: 0.20758575201034546\n",
            "Epoch [119/200], Loss: 0.24650757014751434\n",
            "Epoch [120/200], Loss: 0.28412511944770813\n",
            "Epoch [121/200], Loss: 0.6236450672149658\n",
            "Epoch [122/200], Loss: 0.49226367473602295\n",
            "Epoch [123/200], Loss: 0.4261799156665802\n",
            "Epoch [124/200], Loss: 0.4402552545070648\n",
            "Epoch [125/200], Loss: 0.2730044424533844\n",
            "Epoch [126/200], Loss: 0.21402187645435333\n",
            "Epoch [127/200], Loss: 0.1755771040916443\n",
            "Epoch [128/200], Loss: 0.6807255744934082\n",
            "Epoch [129/200], Loss: 0.23967064917087555\n",
            "Epoch [130/200], Loss: 0.3168744444847107\n",
            "Epoch [131/200], Loss: 0.17993192374706268\n",
            "Epoch [132/200], Loss: 0.30107399821281433\n",
            "Epoch [133/200], Loss: 0.6404641270637512\n",
            "Epoch [134/200], Loss: 0.4028407037258148\n",
            "Epoch [135/200], Loss: 0.2719426453113556\n",
            "Epoch [136/200], Loss: 0.12601368129253387\n",
            "Epoch [137/200], Loss: 0.4659266471862793\n",
            "Epoch [138/200], Loss: 0.12880973517894745\n",
            "Epoch [139/200], Loss: 0.6348013281822205\n",
            "Epoch [140/200], Loss: 0.3139689266681671\n",
            "Epoch [141/200], Loss: 0.4636351764202118\n",
            "Epoch [142/200], Loss: 0.6467990279197693\n",
            "Epoch [143/200], Loss: 0.5163889527320862\n",
            "Epoch [144/200], Loss: 0.26141682267189026\n",
            "Epoch [145/200], Loss: 0.46173325181007385\n",
            "Epoch [146/200], Loss: 0.2441605180501938\n",
            "Epoch [147/200], Loss: 0.21848946809768677\n",
            "Epoch [148/200], Loss: 0.3968442678451538\n",
            "Epoch [149/200], Loss: 0.6728420257568359\n",
            "Epoch [150/200], Loss: 0.2715880572795868\n",
            "Epoch [151/200], Loss: 0.20530009269714355\n",
            "Epoch [152/200], Loss: 0.2905868589878082\n",
            "Epoch [153/200], Loss: 0.2657680809497833\n",
            "Epoch [154/200], Loss: 0.37360286712646484\n",
            "Epoch [155/200], Loss: 0.2832200527191162\n",
            "Epoch [156/200], Loss: 0.24509532749652863\n",
            "Epoch [157/200], Loss: 0.3326202630996704\n",
            "Epoch [158/200], Loss: 0.6884210109710693\n",
            "Epoch [159/200], Loss: 0.23442554473876953\n",
            "Epoch [160/200], Loss: 0.24507524073123932\n",
            "Epoch [161/200], Loss: 0.5069488883018494\n",
            "Epoch [162/200], Loss: 0.4356098175048828\n",
            "Epoch [163/200], Loss: 0.4948079586029053\n",
            "Epoch [164/200], Loss: 0.05507516860961914\n",
            "Epoch [165/200], Loss: 0.5341662764549255\n",
            "Epoch [166/200], Loss: 0.1140063926577568\n",
            "Epoch [167/200], Loss: 0.5717721581459045\n",
            "Epoch [168/200], Loss: 0.12216558307409286\n",
            "Epoch [169/200], Loss: 0.22532276809215546\n",
            "Epoch [170/200], Loss: 0.32628703117370605\n",
            "Epoch [171/200], Loss: 0.3544502556324005\n",
            "Epoch [172/200], Loss: 0.31055089831352234\n",
            "Epoch [173/200], Loss: 0.24447815120220184\n",
            "Epoch [174/200], Loss: 0.3541892468929291\n",
            "Epoch [175/200], Loss: 0.32381829619407654\n",
            "Epoch [176/200], Loss: 0.5600547194480896\n",
            "Epoch [177/200], Loss: 0.296575665473938\n",
            "Epoch [178/200], Loss: 0.2816103994846344\n",
            "Epoch [179/200], Loss: 0.2155637890100479\n",
            "Epoch [180/200], Loss: 0.5406352281570435\n",
            "Epoch [181/200], Loss: 0.6539520025253296\n",
            "Epoch [182/200], Loss: 0.3799239695072174\n",
            "Epoch [183/200], Loss: 0.18450038135051727\n",
            "Epoch [184/200], Loss: 0.1407613307237625\n",
            "Epoch [185/200], Loss: 0.38889262080192566\n",
            "Epoch [186/200], Loss: 0.4791998565196991\n",
            "Epoch [187/200], Loss: 0.23110459744930267\n",
            "Epoch [188/200], Loss: 0.42970526218414307\n",
            "Epoch [189/200], Loss: 0.7306459546089172\n",
            "Epoch [190/200], Loss: 0.31577959656715393\n",
            "Epoch [191/200], Loss: 0.20725344121456146\n",
            "Epoch [192/200], Loss: 0.22321546077728271\n",
            "Epoch [193/200], Loss: 0.4272705614566803\n",
            "Epoch [194/200], Loss: 0.2409009486436844\n",
            "Epoch [195/200], Loss: 0.2925560474395752\n",
            "Epoch [196/200], Loss: 0.45783546566963196\n",
            "Epoch [197/200], Loss: 0.44459664821624756\n",
            "Epoch [198/200], Loss: 0.5640566349029541\n",
            "Epoch [199/200], Loss: 0.2191392034292221\n",
            "Epoch [200/200], Loss: 0.1916636824607849\n"
          ]
        }
      ],
      "source": [
        "hidden_size1 = 128\n",
        "hidden_size2 = 64\n",
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(DeepMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)  # Input to first hidden layer\n",
        "        self.relu1 = nn.ReLU()  # Activation function for the first hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # First hidden layer to second hidden layer\n",
        "        self.relu2 = nn.ReLU()  # Activation function for the second hidden layer\n",
        "        self.fc3 = nn.Linear(hidden_size2, 1)  # Second hidden layer to output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "model = DeepMLP(input_size)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    y_pred = (y_pred >= 0.5).float()\n",
        "    accuracy = (y_pred == y_test.view(-1, 1)).float().mean()\n",
        "    print(f'Accuracy on test set: {accuracy.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5SGzJxI0T2-",
        "outputId": "89534719-a63a-4be5-97bb-2a5cc5e7bc87"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.7402597665786743\n"
          ]
        }
      ]
    }
  ]
}